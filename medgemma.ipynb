{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peterruler/med-gemma/blob/main/medgemma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGbh-xdjk0iH"
      },
      "source": [
        "must have a colab plus subscription (10$ / month), and choose an A100 GPU in the runtime settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoGkZcA7ZCKG",
        "outputId": "333c761e-6c55-44f7-95bc-c3d95df4f969"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.4.26)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.32.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (1.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m109.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.46.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install huggingface_hub\n",
        "!pip install accelerate\n",
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb7opYM5d6JM",
        "outputId": "92d207ab-8817-41df-93a5-502dd16879dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available()) # If you intended to use GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ov63dCtkoPI"
      },
      "source": [
        "must login to huggingface (https://huggingface.co) and create an acesss token with read rights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NpjwGTvCciY1"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(token=hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "87c74f80616749da963572f6113398d7",
            "7b880021f018419781a787c0bcfc0d76",
            "7cba48f4b8c44e5a8d40c5461611d416",
            "f9fcc1f38c4e4793a6ffd6f8b3501c78",
            "1100911bdacf4d56bf61ecd0affd4629",
            "dc05a371542f4b3e959b778750852336",
            "8707c8efba844a0c932b53bb0b2a3732",
            "9bca2818e61f45dfb6febe693eb44853",
            "33ca6d8d5ea24eda8e6dc45e2b6461b6",
            "9cc66ab957d949e9a64c4f66bc609da5",
            "2a4323e078814b7496d1a6f21428af07",
            "f5651585e7df4b65a904b597a04bb973",
            "da0dea422bff4ea8a1169380b1f2d77b",
            "4f3cf9b08fa146ccb70671168f8d727b",
            "70c63fb2144d435da9db32c74aed886e",
            "b5d4a23a19fc4aa1b09c5aa29349cb98",
            "c8f14ab804a04fa698ebdd626cf8acde",
            "c30d8d9dcb494eb4b1703d5098ac3d22",
            "03188cae4bf448b6a141379d53e9a421",
            "5edb92da0f074fe586e8b7d092c029e6",
            "eefdc1b2e432453c9d8a827c4bd27d0d",
            "46f8cc926f8f412c8e7250ac62f1efa4",
            "f95d08f5a7be4e5b991831e850a6bdba",
            "9fbbe7de2eda4ff58fb5b8e6a8d1ec96",
            "8a5c0a4338244e65ad89ae5fbdfe77a9",
            "2ba4361cb5414a48971862562efcafdf",
            "26959ea15ae14ae6b63d7eb61cffd9a7",
            "6da8e7b68762428596fe543a643fcd82",
            "175782de137547dea97d7fa5bd8e7021",
            "e58a5cc26a7646138de2b02cff3c7306",
            "385a6e3005d34c468b4c91ab04794290",
            "eccca2dce90b4a1e821a950d71f0553c",
            "a3ea034f36a54d6b93b33fd5cd55f7aa",
            "643ed84d4dc849d8ac581b52dc4e0bc7",
            "18eefb7df29a4ef6b2a47cc6d4d25ea8",
            "3b15363ec01c44d28d498ac95f92f883",
            "14e3111250d24485bec705fc946782e9",
            "5ceb356a7fc04c6bb944e0b63e89eab8",
            "198e9ad35d274e218d439db761d28dd3",
            "cc860ac1c0174afbac4d321b787da8da",
            "8223db26a949466385e5bf5057cdaeeb",
            "54daffba6f8f4ecfb58cd70717dbfae4",
            "90d198bf526c4a4283e57b1da520c38f",
            "c05dbf3fc8024f549ae8a16381ab5195",
            "398c3445105941878203b1b9900281cd",
            "e280533d5f4c4a1d8ffee43b6bb423d1",
            "a8d82a0c42224bbd984a786cb113d940",
            "b13891344d3649eeb53c56d5e296df2f",
            "060f97eec2134f9ea4ebc5f43edb63f9",
            "ab9656e7026c4a0a86d5efd26c26175b",
            "ccab69c1060f45c3b818a519aeacc920",
            "05695f3bf62048cc9cab1f040950ae2a",
            "88e923932e6948628967e00406d7718a",
            "4dbd3e28a0df4d5da48ade49fb7b7e37",
            "2b4c3b848b6044ad8b32fd556edd9d08",
            "2101f68152e746b18357473d2588e68f",
            "7ce31d5825d542b49e17e55f3a718978",
            "f421d89f03a44c3ea57b9728b0dcbf2b",
            "c9fc651049024e3089474d5ad9278eaf",
            "8d434b0af02546e29a61d1d0068c299f",
            "1db7dfcaf9824fd9ad58b07c0f510e7c",
            "7b30e88a1ec840c4a851f5a2fa2407f3",
            "26de6784d2dc4e17b9b84f0ac56b7a24",
            "c85771ec1426408cb684ef86065a4cf8",
            "a2405888f3174c33a13cf1313be17b21",
            "70fd2f98218d47188c1c76b8854a7c2f",
            "eae49f0265a4465da082e540a56639bb",
            "7df777409ae34fd2a203c95b850aa86a",
            "b0a1f4ae7e52425d8b0608da9d50e4a1",
            "da0ab05184cd44ddad3cf1961eee3b99",
            "e48beec7eddd4eb2857615ac6e3c8eb1",
            "d2b2f258f6e247e7b268788a608688b9",
            "93db7f3608c044bbb963618b8db08f18",
            "34edbef64a70460ba1cdadf09c8e11c6",
            "6007ddb65a8b437ba5330329bf0a59e1",
            "213e423fa22f45c883df9bf8c0bac9b4",
            "cb1ae6dcfcea4dbe90ae7114f1f18604",
            "e11148dd585e4b8b842c5559fda4c4c2",
            "8e10301de525499d848e7ccb6e5d0773",
            "95b7fada4fa4434782e6766c631c34a2",
            "2e35380f969c494595b523579079363d",
            "717a5c225f444667973b8eca83c8bbaa",
            "6294b04f48f74cd8a17786912b36d4fa",
            "a1379233738f49eaaa3269beac828d05",
            "bbade963e13d4ff9bac47eb2b9a440c4",
            "a0d35e4357c64ca2a51006a66e79aebb",
            "3aa8237f4e5d49d08f0fc01b98c57f2c",
            "dcb6d640271d408aa39df41f5201f114",
            "8a4acb5d8b5f4af4a6e77133c40830eb",
            "abc894af73124235a6fc260866304afc",
            "b65dd000de4c43a8abede836e66e0cb4",
            "ae8b8ba4188744e49c91c40b5b661e9c",
            "36a2dd72e5ae4c4dbfa1aab6e6856b16",
            "f371f1e390fe4fb6b3449b6b57ad594a",
            "2867474896e041829b7379d86e73e753",
            "0b929ad9b4ec4fb99a7bbf0de275cb63",
            "774e631e755e499a91458b128c574965",
            "751f9829fb6b4bd0b8f9a86b9449636f",
            "627ea41070c444148c63eea86924d365",
            "3d57b2190166435994d457aef7613850",
            "9564a0d985a34115a649c837439904c9",
            "1e4ef35a0b264bf984761116b5016af3",
            "efaf2b21b35b4128a58367c2bebbf255",
            "1b0460818b2349aa89b537a9b9c8e5ff",
            "922d975abdab46dc85a69ceeddb3b4dd",
            "0ca0c993500c4394ab087313697a3e56",
            "09bbab23135246a7b20a15b1d4e59212",
            "a8c370a3c61840f590f717f50e560216",
            "5f64fc1742624bca8982878942e7cdfa",
            "f574d7c335124882b3e92851a13a784d",
            "0d5baa07ba054438bf0937d6d763ac97",
            "40de22c4ec824d45a5074e4d642896ae",
            "98f520923263475e80573da0cf40cd0d",
            "5ab6c3f3314a4fe38d12a4f0eb82e796",
            "9cba7de6c76840059ed13f8c0acfde9b",
            "e9fe606d1f024f0ab7003f8e163e6bbf",
            "87eb2b971b3a468f8bc9716e9935a401",
            "254fb1d612b9441dbe8872069f28a0f0",
            "7327525ed0ac46f292930fc38a005b70",
            "12a6fd34b0824b42a99885441881e214",
            "3d32bcd3a51b4b01bc81d0b9c75a9af4",
            "3025a538806648a7b36a7dd2f1bdb098",
            "fe45182c16264bbfba106eedd6874a37",
            "25c27f3d77124a93ac0bb4db36f88eef",
            "e3ac46f31efa4d0cb48aeb92b641c98f",
            "b440f61fcc144a42998dafc4cdc7d1d9",
            "61a2e93e7ed34fb5812be4a2d0e7a3ce",
            "7298fb07a6cc43138fb984fb27f7c711",
            "a6815ac65e364f18972ea3b365ba8c1a",
            "e7aafc0b22c34465bc2fd989183af56a",
            "8042e9e4be204804a5eacb00d2b98611",
            "c7903b3ec7004e22bc77abd4bf53f992",
            "05d072460123499bb9924f25e7bd7aea",
            "df2e6087379845e89ba061a2ac056bf9",
            "3e46a590d3e64a5aa23b48a1df1946c3",
            "0f2b64b5d7174c11b4d5df6d8ef439a3",
            "b4b1a5f749c248769203e70a2e4f8ce0",
            "0f57f414b7164c68b8a7ce4b1e2faa3b",
            "d83c50e76e1d47a195d555a08318290b",
            "4155c258b7ee4e8fbb6670adbd366e1b",
            "86b5c27589704024abf1ae4100d30a65",
            "c4646fe6df7849638f7e609f0735f66b",
            "ce6e8d96684f42109bb4601aaab739db",
            "fc80c1aff1894d9fa3a49c42c45defa8",
            "5702d5aff2994b1faec3f400fb5787b7",
            "44fbd4a8384e47188020e37503286f13",
            "0f235fd0c48447728c8e038dc5b87429",
            "b44ea9a6bc3d47578da5105078596d3b",
            "59788f9eb603487a9216a006da379702",
            "e3b0ccc8cb8f442e8835eac18da9610f",
            "6e970ac0ac914b93a84565348f4f3ae8",
            "871bdbfe6bbe4a0688e206489a74044b",
            "ddd156a5bfba4d328ba4d1a4cd31daa2",
            "23462cd37e4f45d8845cdbc43e44a348",
            "07b22ee707054a0fb546c389e2ca6c27",
            "953c266217554c06a1e147111e8ef85a",
            "d5fb266745a440ca8ad029bc86af15c2",
            "5dbabd5be981476aa9219f92bdb5c593",
            "8c2955b439024298901a40360bdf5bc5",
            "a599ff417e444c8cbbba79fda9da5956",
            "606c1913807645239f1a952d3408b9b1",
            "d52d16a5c87245c28137f42c37c228c6",
            "1687faed27a1499da3609a10ca4aa2cd",
            "db8d3dbb19a84a1eb6269ea01f10ba8e",
            "f3c69964a48243559c5186cecc757e0c"
          ]
        },
        "id": "AQR2TzfDgFdS",
        "outputId": "5462057c-e2a2-4d57-a1bd-0b7c1f8e6912"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA is available. Using device: cuda\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87c74f80616749da963572f6113398d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.61k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5651585e7df4b65a904b597a04bb973",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f95d08f5a7be4e5b991831e850a6bdba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "643ed84d4dc849d8ac581b52dc4e0bc7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "398c3445105941878203b1b9900281cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2101f68152e746b18357473d2588e68f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eae49f0265a4465da082e540a56639bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e11148dd585e4b8b842c5559fda4c4c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "processor_config.json:   0%|          | 0.00/70.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a4acb5d8b5f4af4a6e77133c40830eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "chat_template.jinja:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d57b2190166435994d457aef7613850",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d5baa07ba054438bf0937d6d763ac97",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3025a538806648a7b36a7dd2f1bdb098",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05d072460123499bb9924f25e7bd7aea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc80c1aff1894d9fa3a49c42c45defa8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07b22ee707054a0fb546c389e2ca6c27",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipeline loaded successfully using 4-bit quantization.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image loaded successfully.\n",
            "Generated Text:\n",
            "Okay, based on the image provided, here's a description of the X-ray:\n",
            "\n",
            "**Overall Impression:**\n",
            "\n",
            "The image appears to be a standard PA (posterior-anterior) chest X-ray.\n",
            "\n",
            "**Key Features:**\n",
            "\n",
            "*   **Heart Size and Shape:** The heart silhouette is within normal limits.\n",
            "*   **Mediastinum:** The mediastinum (space in the middle of the chest containing the heart, great vessels, trachea, etc.) appears normal in width.\n",
            "*   **Lungs:** The lungs are well-expanded. There is no evidence of consolidation, infiltrates, or significant masses in the lungs.\n",
            "*   **Bones:** The visualized bony structures of the ribs, clavicles, and spine appear intact.\n",
            "*   **Soft Tissues:** The soft tissues appear normal. There is no evidence of obvious subcutaneous emphysema.\n",
            "\n",
            "**Important Considerations:**\n",
            "\n",
            "*   **Image Quality:** This is a single image, and a full evaluation\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "from PIL import Image\n",
        "import requests\n",
        "import os\n",
        "from io import BytesIO\n",
        "\n",
        "# Set environment variable to disable torch compile (kept as in original code)\n",
        "os.environ[\"TORCH_COMPILE_DISABLE\"] = \"1\"\n",
        "\n",
        "# Check if CUDA is available (already done, but good to be explicit)\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"CUDA is not available. Model loading might fail or be very slow on CPU.\")\n",
        "    device = \"cpu\"\n",
        "else:\n",
        "    device = \"cuda\"\n",
        "    print(f\"CUDA is available. Using device: {device}\")\n",
        "\n",
        "# Load the pipeline with 8-bit quantization to reduce memory usage\n",
        "# This requires the bitsandbytes library, which you have already installed.\n",
        "try:\n",
        "    pipe = pipeline(\n",
        "        \"image-text-to-text\",\n",
        "        model=\"google/medgemma-4b-it\",\n",
        "        torch_dtype=torch.bfloat16, # Keep bfloat16 if possible, but 8-bit is the main memory saver\n",
        "        # Add load_in_8bit=True for 8-bit quantization\n",
        "        # If 8-bit still fails, try load_in_4bit=True (requires additional setup sometimes)\n",
        "        model_kwargs={\"load_in_4bit\": True}\n",
        "    )\n",
        "    print(\"Pipeline loaded successfully using 4-bit quantization.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading pipeline with 8-bit quantization: {e}\")\n",
        "    # Fallback to loading without quantization or suggest smaller model if necessary\n",
        "    print(\"Consider trying 4-bit quantization or a smaller model if 8-bit fails.\")\n",
        "    # If you want to try 4-bit as a fallback, uncomment the code below (and ensure you have the necessary setup if needed)\n",
        "    # try:\n",
        "    #     pipe = pipeline(\n",
        "    #         \"image-text-to-text\",\n",
        "    #         model=\"google/medgemma-4b-it\",\n",
        "    #         torch_dtype=torch.bfloat16,\n",
        "    #         device=device,\n",
        "    #         model_kwargs={\"load_in_4bit\": True}\n",
        "    #     )\n",
        "    #     print(\"Pipeline loaded successfully using 4-bit quantization.\")\n",
        "    # except Exception as e4bit:\n",
        "    #     print(f\"Error loading pipeline with 4-bit quantization: {e4bit}\")\n",
        "    #     print(\"Could not load the model with 8-bit or 4-bit quantization.\")\n",
        "    #     exit() # Exit if model cannot be loaded\n",
        "\n",
        "# Image attribution: Stillwaterising, CC0, via Wikimedia Commons\n",
        "image_url = \"https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(image_url, headers={\"User-Agent\": \"example\"}, stream=True)\n",
        "    response.raise_for_status() # Raise an exception for bad status codes\n",
        "    image = Image.open(BytesIO(response.content)).convert(\"RGB\") # Ensure image is in RGB format\n",
        "    print(\"Image loaded successfully.\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error fetching image from URL: {e}\")\n",
        "    exit() # Exit if image cannot be loaded\n",
        "except Exception as e:\n",
        "    print(f\"Error opening image: {e}\")\n",
        "    exit()\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": [{\"type\": \"text\", \"text\": \"You are an expert radiologist.\"}]\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": \"Describe this X-ray\"},\n",
        "            {\"type\": \"image\", \"image\": image}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "# The pipeline expects the messages directly as the primary input.\n",
        "# The `text=` notation is not correct for this pipeline type and message format.\n",
        "try:\n",
        "    output = pipe(messages, max_new_tokens=200) # Pass messages directly\n",
        "\n",
        "    # Process the output to extract the generated text\n",
        "    # The structure of the output can vary depending on the pipeline and model.\n",
        "    # Based on the original code's access, it seems like the generated text is\n",
        "    # a list of message-like segments within output[0][\"generated_text\"].\n",
        "    # We will use a safe approach to access the content of the last segment.\n",
        "    if output and isinstance(output, list) and len(output) > 0 and output[0].get(\"generated_text\") is not None:\n",
        "        generated_content_list = output[0][\"generated_text\"]\n",
        "        if isinstance(generated_content_list, list) and len(generated_content_list) > 0:\n",
        "            final_segment = generated_content_list[-1]\n",
        "            if isinstance(final_segment, dict) and \"content\" in final_segment:\n",
        "                print(\"Generated Text:\")\n",
        "                print(final_segment[\"content\"])\n",
        "            elif isinstance(final_segment, str): # Sometimes it might be a direct string\n",
        "                 print(\"Generated Text:\")\n",
        "                 print(final_segment)\n",
        "            else:\n",
        "                print(\"Could not find 'content' in the last segment of the generated text.\")\n",
        "                print(\"Full output of the last segment:\", final_segment)\n",
        "        else:\n",
        "            print(\"'generated_text' is an empty list.\")\n",
        "            print(\"Full output of output[0]:\", output[0])\n",
        "    else:\n",
        "        print(\"Unexpected output structure from the pipeline.\")\n",
        "        print(\"Full output:\", output)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during pipeline execution: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "F7RRC0xujJpC"
      },
      "outputs": [],
      "source": [
        "from io import BytesIO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbV2GQncZ_MX",
        "outputId": "33de8064-a662-41a3-b695-c6eee2b64d31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    IS_COLAB = True\n",
        "except ImportError:\n",
        "    IS_COLAB = False\n",
        "    print(\"Not running in Google Colab or drive mount failed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VwCvdMGImMq9"
      },
      "outputs": [],
      "source": [
        "# your code with your MRI / CT images\n",
        "# Pfade zu Ihren Bildern auf Google Drive\n",
        "#image_path1 = \"/content/drive/My Drive/dl-udemy/images/wado.jpeg\"\n",
        "#image_path2 = \"/content/drive/My Drive/dl-udemy/images/wado-2.jpeg\"\n",
        "#image_path3 = \"/content/drive/My Drive/dl-udemy/images/wado-3.jpeg\"\n",
        "#image_path4 = \"/content/drive/My Drive/dl-udemy/images/wado-4.jpeg\"\n",
        "#image_path5 = \"/content/drive/My Drive/dl-udemy/images/wado-5.jpeg\"\n",
        "\n",
        "# Bilder direkt von den Dateipfaden laden\n",
        "#try:\n",
        "#    image1 = Image.open(image_path1)\n",
        "#    image2 = Image.open(image_path2)\n",
        "#    image3 = Image.open(image_path3)\n",
        "#    image4 = Image.open(image_path4)\n",
        "#    image5 = Image.open(image_path5)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOpqTHwbXUt+ew0/LYujOpw",
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
